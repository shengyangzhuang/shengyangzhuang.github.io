
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }

    h1 {
        font-weight:300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    tr.spaceUnder>td {
        padding-bottom: 10px;
    }
    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                15px 15px 0 0px #fff, /* The fourth layer */
                15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                20px 20px 0 0px #fff, /* The fifth layer */
                20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                25px 25px 0 0px #fff, /* The fifth layer */
                25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr
    {
        border: 0;
        height: 1.5px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>

<html>
  <head>
        <title>RAIN</title>
        <meta property="og:title" content="sceneflow" />
        <!-- <meta property="og:url" content="https://www.youtube.com/watch?v=cYHQKtBLI3Q" /> -->
  </head>

  <body>
    <br>
    <center>
    <span style="font-size:42px">RAIN: Reinforced Hybrid Attention Inference Network for Motion Forecasting </span>


    <!-- </center> -->
    
    <br>
    <br>
      <table align=center width=800px>

     <tr>
      <span style="font-size:22px"><a href="https://jiachenli94.github.io/">Jiachen Li</a></span><sup>1,2</sup>,&nbsp;&nbsp;
      <span style="font-size:22px"><a href="https://scholar.google.com/citations?user=Gi1y7DAAAAAJ&hl=en&authuser=1">Fan Yang</a></span><sup>3</sup>,&nbsp;&nbsp;
      <span style="font-size:22px"><a href="https://www.linkedin.com/in/hengboma/">Hengbo Ma</a></span><sup>2</sup>,&nbsp;&nbsp;
      <span style="font-size:22px"><a href="https://scholar.google.com/citations?user=TjIKwLcAAAAJ&hl=en">Srikanth Malla</a></span><sup>1</sup>,&nbsp;&nbsp;
      <span style="font-size:22px"><a href="https://me.berkeley.edu/people/masayoshi-tomizuka/">Masayoshi Tomizuka</a></span><sup>2</sup>,&nbsp;&nbsp;
      <span style="font-size:22px"><a href="http://www.chihochoi.me/">Chiho Choi</a></span><sup>1</sup>&nbsp;&nbsp;
   </tr>


     <tr>
       <td align=center colspan="2" style="font-size:22px">
       <center>
       <sup>1</sup>Honda Research Institute USA, Inc.
       </center>
       </td>

      <td align=center colspan="2" style="font-size:22px">
      <center>
      <sup>2</sup>University of California, Berkeley
      </center>
      </td>

      <td align=center colspan="2" style="font-size:22px">
       <center>
       <sup>3</sup>Carnegie Mellon University
       </center>
       </td>

    </table>

        <br> 
    <table align=center width=700px>
       <tr>
        <td align=center width=100px>
        <center>
        <span style="font-size:28px">ICCV 2021 </span>
        </center>
        </td>
     </tr>
    </table>
         <table align=center width=900>
          <tr>
                
                 <span style="font-size:28px">
                <a href="https://jiachenli94.github.io/publications/RAIN/">[Paper](coming soon)</a> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
                <a href="https://jiachenli94.github.io/publications/RAIN/">[Video](coming soon)</a> &nbsp; &nbsp; &nbsp; &nbsp;
                <a href="https://jiachenli94.github.io/publications/RAIN/">[Poster](coming soon)</a> &nbsp; &nbsp; &nbsp; &nbsp; 
                <a href="bibtex.txt">[Bibtex]</a> &nbsp; &nbsp; &nbsp; &nbsp; 

              </span>
              <br>

              </td>
        </tr>
      </table>
            
          
          <br>

          <!-- <hr> -->
          <center><h1>Abstract</h1></center>
          <table align=center width=1000px>
              <tr>
                  <td width=1000px>
<!--                     <center>
                        <img src = "teaser.jpg" height="500px"></img><br>
                  </center> -->
                  <br>
                  <span style="font-size:20px"> Motion forecasting plays a significant role in various domains (e.g., autonomous driving, human-robot interaction), which aims to predict future motion sequences given a set of history observations. However, the observed elements may be of different levels of importance. Some information may be irrelevant or even distracting to the forecasting in certain situations. To address this issue, we propose a generic motion forecasting framework (named RAIN) with dynamic key information selection and ranking based on a hybrid attention mechanism. The general framework is instantiated to handle the tasks of multi-agent trajectory prediction and human motion forecasting, respectively. In the former task, the model learns to recognize the relations between agents with a graph representation and to determine their relative significance. In the latter task, the model learns to capture the temporal proximity and dependency in long-term human motions. We also propose an effective double-stage training pipeline with an alternating training strategy to optimize the parameters in different modules of the framework. We validate the framework on both synthetic simulations and motion forecasting benchmarks in different domains, demonstrating that our method not only achieves state-of-the-art forecasting performance, but also provides interpretable and reasonable hybrid attention weights.
                  </td>
              </tr>

              <tr>
                <td colspan="3"> <br>

                </td>
              </tr>
          </table>


          <hr> <br>
          <center><h1>Key Ideas and Contributions</h1></center>
          <table align=center width=600px>
              <tr>
                  <td width=700px>
                    <br>
                    <center>
                        <img src = "framework.png" height="250px"></img><br>
                  </center>
                  <br><br>
                  <span style="font-size:18px"> <b>Motion Forecasting Framework</b>: We propose a general motion forecasting framework with key information/element selection and ranking based on a novel hybrid attention mechanism. The whole procedure can be iteratively applied over time with a sliding window to enable dynamic selection of key information to adapt to evolving situations. <br><br>
                  <b>Hybrid Attention Mechanism</b>: The hybrid attention mechansim consists of an RL-based hard attention to discriminate key information from complete observations and a soft attention counterpart to further figure out relative significance of the key information. We propose an effective double-stage training pipeline with an alternating training strategy to improve different modules in the framework alternatively. <br><br>
                  </td>
              </tr>

              <tr>
                  <td width=700px>
                    <br>
                    <center>
                        <img src = "model_diagram.png" height="350px"></img><br>
                  </center>
                  <br><br>
                  <span style="font-size:18px"> <b>Multi-Agent Trajectory Forecasting</b>: We instantiate the general framework and propose a novel graph-based model for multi-agent trajectory forecasting. The model consists of three components: graph message passing module (GMP), RL based hard attention module (RL-HA) and soft graph attention based motion generator (SGA-MG), which cooperate closely to improve the final prediction performance. More specifically, for the prediction of a certain target entity, GMP collects information from other entities across graph <em>G</em>. RL-HA discriminates the key relevant elements from the complete observations and provides SGA-MG with an inferred relation graph <em>G'</em> with only selected edges, which is a natural generalization of the traditional hard attention to graph representation. SGA-MG uses soft attention weights to rank the relative importance of key information and generates future trajectories. The prediction together with the ground truth provides rewards to RL-HA during the training phase to guide the improvement of the RL edge selector. GMP is pre-trained to collect contextual information across the whole graph. SGA-MG is pre-trained with a fully connected topology in order to improve training efficiency and stability as well as to enable informative initial reward. <br><br>
                  <b>Human Skeleton Motion Forecasting</b>: We also apply our method to capture long-term temporal dependency in skeleton based human motions. Instead of selecting important or relevant entities, here we propose to utilize the RL-HA module to discriminate key information over the whole history horizon. It can be either frame-wise selection or segment-wise selection. Please refer to the paper for more details about this application. <br><br>
                  </td>
              </tr>

              <tr>
                <td colspan="3"> <br>

                </td>
              </tr>
          </table>


        <hr>
         <!-- <table align=center width=550px> -->
          <br>
                <center><h1>Particle Physics System</h1></center>
                <table align=center width=900px>
                    <tr>
                        <td width=1200px colspan="3">
                          <center>
                              <img src = "particle_plot.png" height="350px"></img><br>
                        </center>
                        </td>
                    </tr>


                    <tr>
                        <td width=600px colspan="3">
                                            <br>
                              <span style="font-size:18px"><b>Visualization of particle trajectories and learned attention maps:</b> The semi-transparent segments represent the observation of 30 time steps and the solid ones represent the prediction of future 50 time steps. In the ground truth relation map, red denotes "repel" and blue denotes "attract". In the attention maps, darker color indicates larger weight. Note that we do not consider self-attention (i.e., zero diagonal). Please refer to the paper for more quantitive results and discussion.
                        </td>
                    </tr>
                </table>
                <br>
      <hr>

      <table align=center width=800>
        <br>
       <center><h1>Vehicle and Pedestrian Trajectory Prediction</h1></center>
          <tr class="spaceUnder">
            <td>
              <img style="width:1050px" src="nuScenes_tables.png"/>
            </td>
          
          </tr>

        <tr>
          <td colspan="3"> 
            <span style="font-size:18px"><b> Quantitative results on nuScenes trajectory prediction benchmark</b>: In this paper, we validated our RAIN framework on the nuScenes dataset, which handles long-term prediction of heterogeneous traffic participants (i.e., vehicles and pedestrians). We predicted the future 4.0s (20 frames) given the history observations of 2.0s (10 frames). The comparison of quantitative results is shown in Table 2 (vehicles) and Table 3 (pedestrians). The experimental results demonstrate that RAIN achieves state-of-the-art performance in terms of minADE and minFDE. Please refer to the paper for more details. <br><br><br>
          </td>
        </tr>

        <tr class="spaceUnder">

            <td>
              <img style="width:1050px" src="nuScenes_plot.png"/>
            </td>
          
          </tr>

        <tr>
          <td colspan="3"> 
            <span style="font-size:18px"><b> The visualization of testing cases in the nuScenes dataset</b>: The light blue dots are history observations, dark blue dots are ground truth, and red dots are predictions with the minimum ADE. The black circles indicate the target agent and gray arrows indicate hybrid attention. The targets only attend to the agents with arrows selected by hard attention and darker colors imply larger weights. There is no arrow in the last case, which implies that the model infers that the target is not influenced by any agent in the scene at current time.
          </td>
        </tr>

      
      </table>
      <br><br>
      <hr>

      <table align=center width=800>
        <br>
       <center><h1>Human Skeleton Motion Forecasting</h1></center>
          <tr class="spaceUnder">
            <td>
              <img style="width:1050px" src="human_motion_table.png"/>
            </td>
          
          </tr>

        <tr>
          <td colspan="3"> 
            <span style="font-size:18px"><b> Quantitive results on the Human3.6M dataset</b>: Among the baselines, HisRepItself yields the previous state-of-the-art performance, which also serves as an ablation model with only soft attention on the motion history. In general, the results show that Ours (hybrid) achieves the best performance both in average and for most actions in terms of both short-term and long-term forecasting accuracy. In particular, Ours (hybrid) outperforms HisRepItself in average, indicating the additional benefits brought by the hard attention mechanism. Please refer to the paper for more details.
          </td>
        </tr>

        <table align=center width=800>
        <br>
          <tr class="spaceUnder">
            <td>
              <img style="width:1050px" src="plot_att_skeleton.png"/>
            </td>
          
          </tr>

        <tr>
          <td colspan="3"> <br>
            <span style="font-size:18px"><b> The visualization of human skeleton motion forecasting of four typical actions with hybrid attention maps</b>: The black skeletons at the bottom are the latest observation sequences which are used to calculate current attention weights. The purple-green skeletons are the prediction hypotheses of our method. The blue-red skeletons are the ground truth. In our experimental setting, for each case there are 31 available history motion segments with a length of 10 frames for the RL hard attention module to select and the soft attention is only applied to the selected segments. In the hybrid attention maps, darker colors indicate larger attention weights. White color means the corresponding motion segment is not selected as key information.
          </td>
        </tr>

        
      </table>
    <br>

    
    <!--
    <hr>
      <center><h1>Code</h1></center>
      <tr>
        <td>
          <span style="font-size:28px">&nbsp;<a href='#'>[GitHub]</a>  (coming soon)
        </td>
      <br>
      -->
      <hr>
            <table align=center width=950px>
                <tr>
                    <td>
                      <left>
                <center><h1>Acknowledgements</h1></center>
                This webpage template was borrowed from some <a href="https://richzhang.github.io/colorization/">colorful folks</a>.
            </left>
        </td>
        </tr>
        </table>

        <br><br>
</body>
</html>
